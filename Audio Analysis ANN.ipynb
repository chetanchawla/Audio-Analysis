{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the audio labels (rating of audio files from 1-5 in the advertisements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels basepath\n",
    "Basepath='C:/Users/asd/Desktop/Research Work/videoAds/'\n",
    "List = open(Basepath+\"labels1000.txt\").readlines()\n",
    "List2=[]#list 2 stores the labels of each audio present to us\n",
    "for i in range(len(List)):\n",
    "    List2.append(List[i].split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,)\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "Labels=np.asarray(List2)#labels are added as numpy array\n",
    "Labels = Labels.astype(np.int)#integer array is made from the strings array\n",
    "print(Labels.shape)\n",
    "print(max(Labels))\n",
    "print(min(Labels))\n",
    "#print(Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion of labels in oneHot format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "hotLabel = np.zeros((len(Labels), max(Labels)+1))\n",
    "hotLabel[np.arange(len(Labels)),Labels] = 1#hotLabel is the oneHot format of the labels\n",
    "print(hotLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the audios we have have been transformed to hold the numeric names for processing using the ExtractAudio code present in this repositry. \n",
    "\n",
    "The names of audios are `1.wav`, `2.wav` and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit=glob.glob(Basepath+'Extract/*.wav')#tit represents the title list of all the audio files we have\n",
    "#print(tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tit2=[]#tit2 is the titles stored in the list\n",
    "for i in range(len(tit)):\n",
    "    tit2.append(tit[i].split('\\\\')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(680,)\n"
     ]
    }
   ],
   "source": [
    "nptit2=np.asarray(tit2)#titles are stored as numpy array of strings\n",
    "print(nptit2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titNo=[]#title number are stored(alternative way) as a list\n",
    "for i in range(len(tit)):\n",
    "    titNo.append(tit2[i].split('.')[0])\n",
    "#print(titNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nptitNo=np.asarray(titNo)\n",
    "intTitNo=[]#integral title numbers are stored here\n",
    "for i in range(len(nptitNo)):\n",
    "    intTitNo.append(int(nptitNo[i]))\n",
    "#print(intTitNo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other way to store the title numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nptitNo = [int(x) for x in nptitNo]\n",
    "nptitNo.sort()\n",
    "#print(nptitNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intTitNo=nptitNo\n",
    "#print(intTitNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalHotLabel=[]\n",
    "for x in intTitNo:\n",
    "    totalHotLabel.append(hotLabel[x])\n",
    "totalHotLabel=np.asarray(totalHotLabel)\n",
    "# print(totalHotLabel[0])\n",
    "# print(\"hi\\n\",hotLabel[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=[]\n",
    "for x in intTitNo:\n",
    "    titles.append(str(x)+\".wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The parser function is used to extract the audios one by one in mfcc(Mel Frequency Cepstrums) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take upto `5 minutes` for `10 audios` depending upon the laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parser(title):\n",
    "#     # function to load files and extract features\n",
    "#     #print(row)\n",
    "#     file_name = os.path.join(Basepath, 'Extract',  title)\n",
    "#     # handle exception to check if there isn't a file which is corrupted\n",
    "#     try:\n",
    "#         # here kaiser_fast is a technique used for faster extraction\n",
    "#         X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "#         # we extract mfcc feature from data\n",
    "#         mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
    "#     except Exception as e:\n",
    "#         print(\"Error encountered while parsing file: \", file_name)\n",
    "#         return None, None\n",
    "#     feature = mfccs\n",
    "# #     label = row.Class\n",
    "#     return feature#, label]\n",
    "# parsed=[]\n",
    "# for i in range(len(titles)):\n",
    "#     parsed.append(parser(titles[i]))\n",
    "# #temp.columns = ['feature', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(parsed[0])\n",
    "# print(totalHotLabel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed2=np.asarray(parsed) #Storing the parsed mfcc format audio files are stored as numpy arrays\n",
    "# print(parsed2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mfcc format audio files are saved here as a pickle file and later, the file is used for extracting the audio informtion to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('Audio1000.txt', 'wb') as file:\n",
    "#      file.write(pickle.dumps(parsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(680, 40)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('Audio1000.txt', 'rb') as f:\n",
    "    temp = pickle.load(f)\n",
    "print(np.asarray(temp).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Training and slicing into validation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(temp[0:600])#Audio files in mfcc format sliced for training\n",
    "y = np.array(totalHotLabel[0:600].tolist())#Labels sliced for training\n",
    "Xvalid = np.array(temp[600:680])#validation audio\n",
    "yvalid = np.array(totalHotLabel[600:680].tolist())#validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 40)\n",
      "(600, 6)\n",
      "(80, 40)\n",
      "(80, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(Xvalid.shape)\n",
    "print(yvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20, input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(20))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "model.fit(X, y, batch_size=30, epochs=50, validation_data=(Xvalid, yvalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 600 samples, validate on 80 samples\n",
    "# Epoch 1/50\n",
    "# 600/600 [==============================] - 1s 2ms/step - loss: 9.5760 - acc: 0.3333 - val_loss: 8.8740 - val_acc: 0.3750\n",
    "# Epoch 2/50\n",
    "# 600/600 [==============================] - 0s 208us/step - loss: 9.3599 - acc: 0.3533 - val_loss: 6.2563 - val_acc: 0.3750\n",
    "# Epoch 3/50\n",
    "# 600/600 [==============================] - 0s 208us/step - loss: 9.2070 - acc: 0.3583 - val_loss: 7.7954 - val_acc: 0.3500\n",
    "# Epoch 4/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 9.1745 - acc: 0.3717 - val_loss: 10.1998 - val_acc: 0.3625\n",
    "# Epoch 5/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 9.1768 - acc: 0.4150 - val_loss: 9.8792 - val_acc: 0.3625\n",
    "# Epoch 6/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 8.9767 - acc: 0.4000 - val_loss: 9.8124 - val_acc: 0.3875\n",
    "# Epoch 7/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.4750 - acc: 0.4367 - val_loss: 8.4497 - val_acc: 0.4000\n",
    "# Epoch 8/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.9892 - acc: 0.3750 - val_loss: 8.2213 - val_acc: 0.4000\n",
    "# Epoch 9/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 9.0653 - acc: 0.3783 - val_loss: 9.3038 - val_acc: 0.3875\n",
    "# Epoch 10/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.9319 - acc: 0.4000 - val_loss: 9.6328 - val_acc: 0.3750\n",
    "# Epoch 11/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 8.8559 - acc: 0.3833 - val_loss: 8.3971 - val_acc: 0.4375\n",
    "# Epoch 12/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 9.0266 - acc: 0.3850 - val_loss: 9.1209 - val_acc: 0.3875\n",
    "# Epoch 13/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 8.7546 - acc: 0.3983 - val_loss: 8.2704 - val_acc: 0.4000\n",
    "# Epoch 14/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.6737 - acc: 0.3950 - val_loss: 7.8393 - val_acc: 0.4250\n",
    "# Epoch 15/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.4143 - acc: 0.3933 - val_loss: 9.7829 - val_acc: 0.3875\n",
    "# Epoch 16/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 8.6687 - acc: 0.4017 - val_loss: 7.3882 - val_acc: 0.4000\n",
    "# Epoch 17/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.2197 - acc: 0.3967 - val_loss: 7.3774 - val_acc: 0.3875\n",
    "# Epoch 18/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.5341 - acc: 0.3850 - val_loss: 9.1661 - val_acc: 0.3750\n",
    "# Epoch 19/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 8.4214 - acc: 0.4100 - val_loss: 9.3875 - val_acc: 0.3875\n",
    "# Epoch 20/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 7.9397 - acc: 0.4067 - val_loss: 7.5088 - val_acc: 0.4125\n",
    "# Epoch 21/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 7.5130 - acc: 0.4167 - val_loss: 7.1436 - val_acc: 0.3750\n",
    "# Epoch 22/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 7.9302 - acc: 0.3767 - val_loss: 6.7042 - val_acc: 0.3625\n",
    "# Epoch 23/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 6.9926 - acc: 0.3750 - val_loss: 5.2409 - val_acc: 0.4125\n",
    "# Epoch 24/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 6.4289 - acc: 0.3750 - val_loss: 5.2665 - val_acc: 0.4000\n",
    "# Epoch 25/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 5.2470 - acc: 0.4050 - val_loss: 2.9997 - val_acc: 0.4250\n",
    "# Epoch 26/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 4.3575 - acc: 0.3133 - val_loss: 1.6546 - val_acc: 0.4000\n",
    "# Epoch 27/50\n",
    "# 600/600 [==============================] - 0s 208us/step - loss: 3.1019 - acc: 0.3000 - val_loss: 1.5328 - val_acc: 0.3875\n",
    "# Epoch 28/50\n",
    "# 600/600 [==============================] - 0s 339us/step - loss: 2.3796 - acc: 0.3300 - val_loss: 1.5869 - val_acc: 0.3375\n",
    "# Epoch 29/50\n",
    "# 600/600 [==============================] - 0s 234us/step - loss: 1.9815 - acc: 0.3400 - val_loss: 1.5891 - val_acc: 0.3750\n",
    "# Epoch 30/50\n",
    "# 600/600 [==============================] - 0s 339us/step - loss: 1.7521 - acc: 0.3400 - val_loss: 1.5959 - val_acc: 0.3250\n",
    "# Epoch 31/50\n",
    "# 600/600 [==============================] - 0s 260us/step - loss: 1.6638 - acc: 0.4050 - val_loss: 1.6127 - val_acc: 0.3125\n",
    "# Epoch 32/50\n",
    "# 600/600 [==============================] - 0s 312us/step - loss: 1.6730 - acc: 0.4050 - val_loss: 1.5836 - val_acc: 0.3375\n",
    "# Epoch 33/50\n",
    "# 600/600 [==============================] - 0s 260us/step - loss: 1.7081 - acc: 0.3617 - val_loss: 1.5636 - val_acc: 0.3625\n",
    "# Epoch 34/50\n",
    "# 600/600 [==============================] - 0s 208us/step - loss: 1.6002 - acc: 0.3900 - val_loss: 1.5442 - val_acc: 0.3625\n",
    "# Epoch 35/50\n",
    "# 600/600 [==============================] - 0s 286us/step - loss: 1.4700 - acc: 0.3650 - val_loss: 1.4980 - val_acc: 0.3375\n",
    "# Epoch 36/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 1.5017 - acc: 0.3883 - val_loss: 1.4879 - val_acc: 0.4000\n",
    "# Epoch 37/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.4762 - acc: 0.4150 - val_loss: 1.4869 - val_acc: 0.3875\n",
    "# Epoch 38/50\n",
    "# 600/600 [==============================] - 0s 208us/step - loss: 1.4553 - acc: 0.4033 - val_loss: 1.4732 - val_acc: 0.3625\n",
    "# Epoch 39/50\n",
    "# 600/600 [==============================] - 0s 130us/step - loss: 1.4314 - acc: 0.4083 - val_loss: 1.4674 - val_acc: 0.3625\n",
    "# Epoch 40/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.4280 - acc: 0.4083 - val_loss: 1.4594 - val_acc: 0.3875\n",
    "# Epoch 41/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.4319 - acc: 0.3950 - val_loss: 1.4565 - val_acc: 0.3875\n",
    "# Epoch 42/50\n",
    "# 600/600 [==============================] - 0s 182us/step - loss: 1.4195 - acc: 0.4367 - val_loss: 1.4551 - val_acc: 0.4000\n",
    "# Epoch 43/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.4240 - acc: 0.4167 - val_loss: 1.4615 - val_acc: 0.3875c\n",
    "# Epoch 44/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.3902 - acc: 0.4317 - val_loss: 1.4554 - val_acc: 0.3750\n",
    "# Epoch 45/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.3972 - acc: 0.4217 - val_loss: 1.4528 - val_acc: 0.3750\n",
    "# Epoch 46/50\n",
    "# 600/600 [==============================] - 0s 156us/step - loss: 1.4279 - acc: 0.4100 - val_loss: 1.4492 - val_acc: 0.3500\n",
    "# Epoch 47/50\n",
    "# 600/600 [==============================] - 0s 260us/step - loss: 1.4258 - acc: 0.3767 - val_loss: 1.4515 - val_acc: 0.3625\n",
    "# Epoch 48/50\n",
    "# 600/600 [==============================] - 0s 313us/step - loss: 1.3783 - acc: 0.4417 - val_loss: 1.4471 - val_acc: 0.3625\n",
    "# Epoch 49/50\n",
    "# 600/600 [==============================] - 0s 365us/step - loss: 1.4088 - acc: 0.4100 - val_loss: 1.4506 - val_acc: 0.3625\n",
    "# Epoch 50/50\n",
    "# 600/600 [==============================] - 0s 339us/step - loss: 1.3430 - acc: 0.4383 - val_loss: 1.4476 - val_acc: 0.3625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.fit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
